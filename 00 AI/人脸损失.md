[ArcFace，CosFace，SphereFace，三种人脸识别算法的损失函数的设计 —— 知乎](https://zhuanlan.zhihu.com/p/285598652)
[人脸识别损失函数简介与 Pytorch 实现：ArcFace、SphereFace、CosFace](https://www.cvmart.net/community/detail/1430)


### 基础知识
**交叉熵**：度量两个概率分布的差异

**信息量**：一个事件发生的概率越大，携带的信息量越小，发生的概率越小，这个事件携带的信息量越大。
$$I(x_0) = -log(p(x_0))$$

**信息熵**：信息量的期望。
$$H(p,q) = -\sum_{i=1}^n p(x_i)log(q(x_i))$$
事件的概率不均，信息熵较小，若各个事件发生的概率一样，信息熵较大。

**相对熵**，KL散度：
$$D_{KL}(p||q) = \sum_{i=1}^n p(x_i)log(\frac{p(x_i)}{q(x_i)})$$ 
目标：计算用 *P* 描述目标问题，比 *Q* 描述目标问题能获得的信息增量。
如果分布P和分布Q是一样的，那么相对熵是0，如果不一样，相对熵大于0，越大，表示两种分布之间的差距越大。
在机器学习的项目中，通常P表示真实的分布，即需要训练模型达到的分布，Q是现在的模型预测的分布。

**交叉熵**，将相对熵变形：
$$
\begin{aligned}
D_{KL}(p||q) &= \sum_{i=1}^n p(x_i)log(\frac{p(x_i)}{q(x_i)}) \\
&= \sum_{i=1}^n p(x_i)log(p(x_i)) - \sum_{i=1}^n p(x_i)log(q(x_i)) \\
&= -H(p(x)) + [-\sum_{i=1}^n p(x_i)log(q(x_i))]
\end{aligned}
$$
前半部分是信息熵的负值，后半部分则是交叉熵，所以交叉熵的公式是:
$$H(p,q) = -\sum_{i=1}^n p(x_i)log(q(x_i))$$
因为P的信息熵是一定的，那么其实是可以省略这部分计算的，交叉熵和相对熵的意义是一样的。只是最后计算出的值，区间不一样。



### SoftmaxLoss & CELoss

**Softmax**
Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量 $z$ “压缩”到另一个K维实向量 $\sigma(z)$ 中，使得每一个元素的范围都在 $\sigma(z)$ 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：
$$\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}} for \ j = 1,...K$$
简单来说 softmax 将一组向量进行压缩，使得到的向量各元素之和为 1，而压缩后的值便可以作为置信率，所以常用于分类问题。另外，在实际运算的时候，为了避免上溢和下溢，在将向量丢进softmax之前往往先对每个元素减去其中最大值，即：
$$\sigma(z)_j = \frac{e^{z_j-z_{max}}}{\sum_{k=1}^Ke^{z_k-z_{max}}} for \ j = 1,...K$$
一般分类任务流程：
```
[image] --> 深度卷积神经网络 --> fc --> [特征向量embedding] --> softmax --> [C类分数]
```


**Softmax Loss**：
$$SL = -\sum_{k=1}^K y_k log(\sigma_k)$$
其中 $y$ 是一个长度为 $K$ 的one-hot向量，即 $y_k \in \{0,1\}$ ，只有ground truth对应的 $y_k = 1$ 。所以也可以简写为：
$$SL = - y_{gt} log(\sigma_{gt})$$

**交叉熵损失**
$$CE = \sum_{k=1}^K -P_klog(p_k)$$
其中 $P$ 是真实分布，在分类任务中， $P$ 实际上等价于上面的 $y$ 。而 $p$ 则是预测分布，在分类任务中 $p$ 实际上等价于上面的 $\sigma$ 。所以：$CE = SL$
$$Softmax Loss = CrossEntropy(Softmax)$$


### SphereFace & ArcFace & CosFace

增强 Softmax 分类能力：
- 类内聚敛(Intra-class compactness)
- 类间分离(Inter-class discrepancy)

Softmax Loss可以转变形式为：
$$
\begin{aligned}
L_2 
&= -log(\sigma_{gt}) = -log(\frac{e^{z_{gt}}}{\sum_{k=1}^K e^{z_k}}) \\
&= -log(\frac{e^{W^T_{gt}x + b_{gt}}}{\sum_{k=1}^K e^{W^T_{k}x + b_{k}}}) \\
&= -log(\frac{e^{||W_{gt}||\ ||x||\ cos(\theta_{W_{gt},x}) + b_{gt}}}{\sum_{k=1}^K e^{||W_{k}||\ ||x||\ cos(\theta_{W_{k},x}) + b_{k}}}) \\
\end{aligned}
$$
其中 $\theta_{i,j} \in (0,\pi)$ 代表两个向量 $i, j$ 之间的夹角

#### SphereFace
对 $W_k$ 归一化，将偏置 $b$ 置为0，即 $||W_j|| = 1 \ and \ b_k = 0$ , 则有：
$$
L_{modified} = -log\dfrac{e^{s\ cos(\theta_{W_{gt},x})}}{\sum_{k=1}^K e^{s\ cos(\theta_{W_{k},x})}}
$$

对于 $\theta$ 乘上一个大于等于 1 的整数 $m$ :
$$
L_{modified} = -log(\dfrac{e^{s\ cos(m\theta_{W_{gt},x})}}{\sum_{k=1}^K e^{s\ cos(m\theta_{W_{k},x})}})
, m \in \{1,2,...\}
$$


#### CosFace
令  $||x|| = s$




### 代码实现
因为是由 Softmax Loss 改进而来，将原本的 fc 层的权重相乘改变为了余弦距离，所以代码实际上替换的是 fc 层，并不是 loss
#### SphereFace
```python
# SphereFace
class SphereProduct(nn.Module):
    r"""Implement of large margin cosine distance: :
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        m: margin
        cos(m*theta)
    """

    def __init__(self, in_features, out_features, m=4):
        super(SphereProduct, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.m = m
        self.base = 1000.0
        self.gamma = 0.12
        self.power = 1
        self.LambdaMin = 5.0
        self.iter = 0
        self.weight = Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform(self.weight)

        # duplication formula
        # 将x\in[-1,1]范围的重复index次映射到y\in[-1,1]上
        self.mlambda = [
            lambda x: x ** 0,
            lambda x: x ** 1,
            lambda x: 2 * x ** 2 - 1,
            lambda x: 4 * x ** 3 - 3 * x,
            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,
            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x
        ]
        """
        执行以下代码直观了解mlambda
        import matplotlib.pyplot as  plt

        mlambda = [
            lambda x: x ** 0,
            lambda x: x ** 1,
            lambda x: 2 * x ** 2 - 1,
            lambda x: 4 * x ** 3 - 3 * x,
            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,
            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x
        ]
        x = [0.01 * i for i in range(-100, 101)]
        print(x)
        for f in mlambda:
            plt.plot(x,[f(i) for i in x])
            plt.show()
        """

    def forward(self, input, label):
        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))
        self.iter += 1
        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))

        # --------------------------- cos(theta) & phi(theta) ---------------------------
        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))
        cos_theta = cos_theta.clamp(-1, 1)
        cos_m_theta = self.mlambda[self.m](cos_theta)
        theta = cos_theta.data.acos()
        k = (self.m * theta / 3.14159265).floor()
        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k
        NormOfFeature = torch.norm(input, 2, 1)

        # --------------------------- convert label to one-hot ---------------------------
        one_hot = torch.zeros(cos_theta.size())
        one_hot = one_hot.cuda() if cos_theta.is_cuda else one_hot
        one_hot.scatter_(1, label.view(-1, 1), 1)

        # --------------------------- Calculate output ---------------------------
        output = (one_hot * (phi_theta - cos_theta) / (1 + self.lamb)) + cos_theta
        output *= NormOfFeature.view(-1, 1)

        return output

    def __repr__(self):
        return self.__class__.__name__ + '(' \
               + 'in_features=' + str(self.in_features) \
               + ', out_features=' + str(self.out_features) \
               + ', m=' + str(self.m) + ')'
```