# 损失函数/目标函数 Objective Functions

## 无监督 Unsupervised

### 交叉熵 Cross-Entropy

$L = -\sum\limits_{i=1}^{n}y \cdot log_2 \hat{y}$

- $y$: 真实分类，标签值

- $\hat{y}$: 模型预测结果，包含了属于每种标签的概率



pytorch中的`cross_entropy(input, target)`函数：

- 输入：

  - input: (batch_size, class)。模型预测结果

  - target: (batch_size)  标签，每个样本的真实值

- 输出：

  - L：交叉熵的值。

对应到损失函数中：

-  $\hat{y}$ 是模型输出的logit也就是预测结果，需要经过softmax后所有类别的预测值相加变为1了

-  $y$ 仅仅是一个标签，所以变成 one-hot 编码之后，才对应了每一类，可以参与计算



### L2损失,均方误差(MSE),二次损失 II Loss

$MSE = \dfrac{\sum\limits_{i=1}^{n}(y_i - y_i^p)^2}{n}$

最常用的回归损失函数，是目标变量和预测值的差值平方和



### Center Loss

参考博客：http://t.csdn.cn/r6Pvw

Center Loss 使学到的特征差异化更大，通常与 Softmax Loss 联合使用。

- Softmax Loss：保证类之间的 feature 距离最大
  - $\mathcal{L}_S = - \sum\limits_{i=1}^m log \dfrac{e^{W^T_{y_i}x_i + b_{y_i}}}{\sum_{j=1}^n e^{W^T_j x_i+b_j}}$

- Center Loss：保证类内的 feature 距离最小，更接近于类中心
  - $\mathcal{L}_C = \frac{1}{2}\sum\limits_{i=1}^m||x_i - c_{y_i}||_2^2$



`m`: mini-batch

`n`: class

$c_{y_i}$: $y_i$ 类对应的中心。



### 类锚聚类损失 CAC Loss

计算特征到各个类中心距离d

$d = e(z,C) = (||||)^T$





