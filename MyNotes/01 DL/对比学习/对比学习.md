### 定义
对比学习顾名思义就是对比着去学习。假设有三张图片A，B，C。AB是猫，C是狗。模型不需要知道AB是猫C是狗，只需要知道AB类似而AB和C不类似。

具体来说，当ABC经过网络模型 $M$ 得到三个特征 $f_1, f_2, f_3$ 时，在特征空间中 $f_1, f_2$ 是尽可能相近，而 $f_3$ 尽可能远离 $f_1, f_2$


### 无监督-代理任务
但是目前为止，虽然不需要知道具体的标签信息，但是还需要知道什么是类似的，什么是不类似的。这样并不能称作是无监督学习。但是在视觉领域，对比学习被认为是无监督学习。

这是因为设置了一些巧妙的[[代理任务]] pretext task，人为订立一些规则。这些规则可以定义哪些相似，哪些不相似。从而提供监督信号去训练模型。

#### Instance discrimination 个体判别
假设一个数据集 $N$ 中有 n 张图片： $x_1, x_2, ..., x_n$. 其中对 $x_i$ 随机裁剪、数据增广进行 Transformation，得到 $x_i^1, x_i^2$.则认为：$x_i$ 与 $x_i^1, x_i^2$ 相似，与其他 $x_j,j \ne i$ 都不相似。
- 正样本: $x_i^1, x_i^2$ 
- 负样本: $x_j$ 
![[对比学习.canvas]]
所以在这个代理任务上，每个图片都被认为是一个类。那么之后将正、负样本送进模型，使用一些常见的对比学习目标函数即可，例如 NCE Loss。

### 灵活性
对比学习，只要找到一种方式，定义正负样本即可。其他操作都遵循基本流程。所以可以应用到很多领域。



### 代码

```python
class SupConLoss(nn.Module):

    def __init__(self, temperature=0.5, scale_by_temperature=True):
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.scale_by_temperature = scale_by_temperature

    def forward(self, features, labels=None, mask=None):
        """
        输入:
            features: 输入样本的特征，尺寸为 [batch_size, hidden_dim].
            labels: 每个样本的ground truth标签，尺寸是[batch_size].
            mask: 用于对比学习的mask，尺寸为 [batch_size, batch_size], 如果样本i和j属于同一个label，那么mask_{i,j}=1 
        输出:
            loss值
        """
        device = (torch.device('cuda')
                  if features.is_cuda
                  else torch.device('cpu'))
        features = F.normalize(features, p=2, dim=1)
        batch_size = features.shape[0]
        # 关于labels参数
        if labels is not None and mask is not None:  # labels和mask不能同时定义值，因为如果有label，那么mask是需要根据Label得到的
            raise ValueError('Cannot define both `labels` and `mask`') 
        elif labels is None and mask is None: # 如果没有labels，也没有mask，就是无监督学习，mask是对角线为1的矩阵，表示(i,i)属于同一类
            mask = torch.eye(batch_size, dtype=torch.float32).to(device)
        elif labels is not None: # 如果给出了labels, mask根据label得到，两个样本i,j的label相等时，mask_{i,j}=1
            labels = labels.contiguous().view(-1, 1)
            if labels.shape[0] != batch_size:
                raise ValueError('Num of labels does not match num of features')
            mask = torch.eq(labels, labels.T).float().to(device)
        else:
            mask = mask.float().to(device)
        '''
        示例: 
        labels: 
            tensor([[1.],
                    [2.],
                    [1.],
                    [1.]])
        mask:  # 两个样本i,j的label相等时，mask_{i,j}=1
            tensor([[1., 0., 1., 1.],
                    [0., 1., 0., 0.],
                    [1., 0., 1., 1.],
                    [1., 0., 1., 1.]]) 
        '''
        # compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(features, features.T),
            self.temperature)  # 计算两两样本间点乘相似度
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()
        exp_logits = torch.exp(logits)
        '''
        logits是anchor_dot_contrast减去每一行的最大值得到的最终相似度
        示例: logits: torch.size([4,4])
        logits:
            tensor([[ 0.0000, -0.0471, -0.3352, -0.2156],
                    [-1.2576,  0.0000, -0.3367, -0.0725],
                    [-1.3500, -0.1409, -0.1420,  0.0000],
                    [-1.4312, -0.0776, -0.2009,  0.0000]])       
        '''
        # 构建mask 
        logits_mask = torch.ones_like(mask) - torch.eye(batch_size)     
        positives_mask = mask * logits_mask
        negatives_mask = 1. - mask
        '''
        但是对于计算Loss而言，(i,i)位置表示样本本身的相似度，对Loss是没用的，所以要mask掉
        # 第ind行第ind位置填充为0
        得到logits_mask:
            tensor([[0., 1., 1., 1.],
                    [1., 0., 1., 1.],
                    [1., 1., 0., 1.],
                    [1., 1., 1., 0.]])
        positives_mask:
        tensor([[0., 0., 1., 1.],
                [0., 0., 0., 0.],
                [1., 0., 0., 1.],
                [1., 0., 1., 0.]])
        negatives_mask:
        tensor([[0., 1., 0., 0.],
                [1., 0., 1., 1.],
                [0., 1., 0., 0.],
                [0., 1., 0., 0.]])
        '''        
        num_positives_per_row  = torch.sum(positives_mask , axis=1) # 除了自己之外，正样本的个数  [2 0 2 2]       
        denominator = torch.sum(
        exp_logits * negatives_mask, axis=1, keepdims=True) + torch.sum(
            exp_logits * positives_mask, axis=1, keepdims=True)  
        
        log_probs = logits - torch.log(denominator)
        if torch.any(torch.isnan(log_probs)):
            raise ValueError("Log_prob has nan!")
        

        log_probs = torch.sum(
            log_probs*positives_mask , axis=1)[num_positives_per_row > 0] / num_positives_per_row[num_positives_per_row > 0]
        '''
        计算正样本平均的log-likelihood
        考虑到一个类别可能只有一个样本，就没有正样本了 比如我们labels的第二个类别 labels[1,2,1,1]
        所以这里只计算正样本个数>0的    
        '''
        # loss
        loss = -log_probs
        if self.scale_by_temperature:
            loss *= self.temperature
        loss = loss.mean()
        return loss
```



> [别出心裁！不用负样本的对比学习](https://xie.infoq.cn/article/fb14904f72c5b74e598fbaceb)
> [再谈对比学习：更好的对比样本选择，更好的对比效果 - CSDN](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/120984199)
> [一文轻松入门对比学习(附代码详解) - zhihu](https://zhuanlan.zhihu.com/p/442415516)

