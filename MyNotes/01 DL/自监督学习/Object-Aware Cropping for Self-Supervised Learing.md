- tag: #DL/Self-supersived 
- 原文: https://arxiv.org/abs/2112.00319
- Code: https://github.com/shlokk/object-cropping-ssl
- 展示: [[MyNotes/01 DL/自监督学习/Object-Aware Cropping总结]]



## 用于自监督学习的对象感知裁剪
![image.png](https://s1.vika.cn/space/2023/02/09/46753a8801a045c8b689d7aafef81d04)

### Abstract
最近自监督学习成功的一个核心部分是裁剪数据的增强，它选择图像的子区域作为自监督损失中的正视图。其基本假设是，==随机裁剪和调整大小的给定图像区域共享有关感兴趣的对象的信息，而学习到的表征将捕获这些信息。==这一假设在ImageNet等数据集中大多得到满足，在这些数据集中，有一个大型的、居中的物体，它极有可能出现在随机裁剪的完整图像中。然而，在其他数据集中，如OpenImages或COCO，它们更能代表现实世界中未经整理的数据，图像中通常有多个小物体。在这项工作中，我们表明，基于通常的随机裁剪的自我监督学习在这类数据集上表现不佳。我们建议用从物体提议算法中获得的crop来取代一个或两个随机crop。这鼓励模型同时学习物体和场景层面的语义表征。使用这种方法，我们称之为对象意识裁剪，在分类和对象检测基准上比场景裁剪有明显的改善。例如，在OpenImages上，我们的方法比使用基于MoCo-v2的预训练的随机场景级裁剪实现了8.8%的mAP改进。我们还显示了在COCO和PASCAL-VOC物体检测和分割任务上比最先进的自监督学习方法有明显的改进。我们的方法高效、简单、通用，可用于大多数现有的对比性和非对比性自监督学习框架。


### 1 Introduction
在最近关于图像表征的自我监督学习（SSL）的工作中，最成功的方法是将数据增强作为一个重要工具（Chen等人，2020a；He等人，2019；Grill等人，2020；Tian等人，2019；Caron等人，2020）。给定一个随机选择的图像样本，使用常见的图像变换，如裁剪和调整图像的较小区域、颜色变换（色调、饱和度、对比度）、旋转等，生成图像的增强（Chen等人，2020a；Gidaris等人，2018）。在这些增强措施中，使用裁剪似乎显然是最强大的（见Chen等人（2020a），图5）。这有直观的意义：==裁剪后再调整大小，迫使表征集中在具有不同长宽比的物体的不同部分。这使得该表征对诸如比例和遮挡等自然变化具有鲁棒性==。这个方案的隐含假设是，感兴趣的对象（分类或检测目标）占据了图像的大部分，并且在图像中相当居中，因此，随机裁剪图像大多会导致（大部分）对象仍然存在于被裁剪的图像中。这样的假设对于ImageNet（Krizhevsky等人，2012）这样的 "标志性 "数据集是成立的。迫使所产生的表征更接近，可以最大限度地提高裁剪（也称为视图）之间的相互信息（van den Oord等人，2018；Tian等人，2019）。

然而，在 "非标志性 "数据集的情况下，如OpenImages（Kuznetsova等人，2020年）和COCO（Lin等人，2014年），感兴趣的对象相对于图像大小来说很小，而且很少居中，见图1。这些数据集更能代表真实世界的未处理数据。我们发现默认的随机裁剪方法（我们称之为场景裁剪）导致自监督的对比学习方法的性能大幅下降。例如，使用MoCo-v2（Chen等人，2020b）的默认框架，与完全监督学习相比，有16.5%的平均精度（mAP）的差距。其他最先进的方法，如BYOL（Grill等人，2020）、SwAV（Caron等人，2020）和CMC（Tian等人，2019）也表现不佳（见表1）。正如我们所展示的，这里的核心问题是，==随机场景crop不包含足够的物体信息，导致表示质量下降。==
![image.png](https://s1.vika.cn/space/2023/02/08/700a0847f3234378815a50966dc35913)

然而，仅仅从场景层面的裁剪切换到纯粹的物体层面的裁剪并不能利用大多数自然图像中场景和物体之间存在的相关性。这些相关性对下游任务是有帮助的（Xiao等人，2020）。考虑到这一点，我们引入了物体感知裁剪，它使用BING算法（Cheng等人，2014）进行了简单的预处理步骤。BING输出多个对象建议，我们随机挑选其中一个作为SSL损失的第一个视图（SSL的大多数变体使用同一样本的至少两个视图作为 "正面"）。对于第二个视图，我们试验了以下变体，以将物体和场景信息纳入表示：（1）场景级随机裁剪，以纳入物体和场景背景（我们称这种设置为obj-scene）；（2）BING建议的扩张版本，然后对两个视图进行随机裁剪（obj-obj+dilate）；（3）第二次裁剪，对BING建议进行随机转移（obj-obj+shift）。基线将场景级的随机裁剪应用于两个视图（场景-场景）。

我们进行了一些包含对象感知裁剪的实验，发现在不同的数据集和任务中，MoCo-v2（Chen等人，2020b）、BYOL（Grill等人，2020）和Dense-CL Wang等人（2021）等最先进的自监督方法有一致的改进（见图2和第4节）。这种方法速度快（在NVIDIA P100 GPU上大于125帧），对基线计算时间的开销最小（<1%），而且实施起来很简单。我们已经发布了数据集： https://github.com/shlokk/object-cropping-ssl
![image.png](https://s1.vika.cn/space/2023/02/08/ac469aa36ac64a0eaf468beb573f2398)



### 2 在openImages数据集上对自监督学习方法的分析
在本节中，我们首先确定最先进的SSL方法的一些局限性，如在非iconic数据集上的MoCo-v2（He等人，2019；Chen等人，2020b）、SwAV（Caron等人，2020）和BYOL（Grill等人，2020）。在ImageNet上进行预训练和线性探测时，这些方法几乎缩小了与监督学习方法的性能差距（Deng等人，2009）。然而，这些方法在非标志性数据集（图像包含多个小物体）上的表现还没有得到广泛的研究。OpenImages（Kuznetsova等人，2020年）就是这样一个数据集，其中包含复杂场景和多个物体的图像（平均每张图像包含8个注释的物体）。它由总共910万张图像组成。为了对剪裁对SSL方法性能的有效性进行控制性实验，我们构建了OpenImages数据集的一个子集。我们对有标记边界框的图像进行采样，以便与完全监督学习进行比较。其次，我们对至少有两个不同类别的物体的图像进行抽样，以创建一个能更好地反映现实世界中未经整理的数据的数据集。最后，我们只考虑至少有900张图片的类别，以减轻类别分布不平衡的影响。经过这样的处理，我们有212,753张图片存在于208个类别中，平均每张图片有大约12个物体。我们在补充文件A中提供了进一步的细节。为了方便起见，我们在本文中仍然把这个数据集称为OpenImages。

#### 2.1 SSL方法的性能
我们在OpenImages数据集上对几种SSL方法MoCo-v2、CMC（Tian等人，2019）、SwAV（Caron等人，2020）和BYOL（Grill等人，2020）进行预训练。MoCo-v2、BYOL和其他最近的最先进的SSL方法都是依靠同一图像的场景-场景裁剪来产生正样本。这种裁剪策略也被用作监督学习中的默认数据增量（He等人，2015；Krizhevsky等人，2012；Cubuk等人，2018）。最常见的工作流程是：选择一个随机比例（默认范围0.2到1.0）和一个随机长宽比（默认范围0.75到1.33）。使用这两个值（比例和长宽比）对原始图像进行裁剪。最后，裁剪的大小被调整为224×224像素的最终尺寸。

我们使用ResNet-50（He等人，2015）深度网络作为所有实验的backbone。在预训练后，我们冻结backbone权重，并在下游数据集和任务上训练线性分类器或进行微调。我们还以完全监督的方式，使用地面真实标签和多类逻辑回归损失来训练一个随机初始化的ResNet-50网络。我们遵循论文（Veit等人，2017）第4.2节中描述的mAP指标（公式(4)）。表1显示了我们的结果。在OpenImages数据集上，我们看到完全监督训练和SSL方法之间的性能差异很大，在所考虑的4种SSL方法中，平均差距为16.3个mAP点。在ImageNet上，第1名的准确性差距要小得多，平均差距只有8.5，几乎是OpenImages的一半。最后一行显示了通过在MOCO-v2使用我们的对象感知裁剪方法（我们将在下一节介绍）获得的显著提升。在OpenImages上，SSL和监督训练之间的差距现在与ImageNet相同。
![image.png](https://s1.vika.cn/space/2023/02/08/02f5353af29647e5b51f7b1aa69ba712)

#### 2.2 分析和动机
我们进行了进一步的实验，以更好地分析表1中的结果，并激励我们提出的方法。我们的实验有助于缩小场景裁剪的范围，这是SSL在OpenImages上表现不佳的主要原因之一，而不是与ImageNet的其他差异，如物体大小、类别分布或图像分辨率。

**物体大小：** 我们将MoCo-v2的性能与完全监督学习的性能进行比较，两种方法都使用基于场景的裁剪。图3（左）显示，对于OpenImages中不同尺寸的物体，监督学习和SSL方法之间的性能差距并不明显。这表明，一旦物体大小低于场景裁剪倾向于忽略物体信息的阈值，MoCo-v2的性能就基本不受物体尺度的影响。
![image.png](https://s1.vika.cn/space/2023/02/08/c59a4ed1982a486abf0df22849329f91)

**长尾分布：** 即使在每个类别选择了至少900张图片，我们的OpenImages子集在每个类别的图片数量上也有很大的变化（从大约1000到60000）。图3（右）描绘了MoCo-v2和监督训练的性能与每类实例数量的关系。我们没有看到相对性能随着类中实例数量的变化而发生明显变化。这就排除了分布的长尾是导致MoCo-v2在OpenImages上的绝对性能不佳的原因。

**ImageNet预训练有帮助吗？** 我们在ImageNet上预训练了一个监督模型，然后在OpenImages数据集上微调了最后的全连接层。从表2的第二栏我们可以看到，这种预训练并不能帮助缩小性能差距。ImageNet预训练没有帮助的原因之一是，OpenImages和ImageNet的类分布明显不同（例如，详细分析见Li等人（2019））。
![image.png](https://s1.vika.cn/space/2023/02/08/a3a36f0f75e94c248654d86a743dc17e)

**调整图像的大小有帮助吗？** 我们还试验了将OpenImages中的图像调整到与ImageNet相同的近似尺寸（384×384）和长宽比。结果显示在表2的第三栏，证实了控制图像大小并不能帮助缩小差距。

**对 ground truth 物体进行裁剪有帮助吗？** 我们从表2的第四栏中看到，对 ground truth 对象框的随机裁剪也无助于缩小性能差距。正如我们在后面的第3节中所表明的，从物体和背景中学习对于在多物体数据集上学习语义信息是很重要的。

**改变随机调整大小的crop的下限比例。** MoCo-v2（Chen等人（2020b））使用了场景裁剪，其大小从ImageNet图像大小（384×384）的20%到100%的均匀分布中选择。由于OpenImages的图像更大，而且与ImageNet相比，物体所占的部分通常更小，所以我们改变了场景裁剪的下限来衡量其影响。表2的最后六列显示，改变场景裁剪的范围不足以缩小性能差距。

我们的结论是，监督训练和SSL训练之间的性能差距可能是由于==数据增强==，而不是图像分布的特点。进一步的分析实验在补充文件（C节）中提供。

### 3 proposal 的方法
我们的分析表明，基于纯粹的物体裁剪或纯粹的场景裁剪的SSL方法，都比监督学习的表现差。这表明了第三种选择：将物体和场景信息都纳入裁剪中。我们的假设是，由于场景和物体之间的自然关联，物体周围的环境有助于学习稳健的表征（Hinton，2021）。然而，目前还不清楚什么样的物体和场景信息组合对学习稳健的表征最有帮助。我们的主要贡献是用一种简单有效的方法来实现这种裁剪机制。我们还试验了一些合理的基线来纳入场景和物体信息。

为了实现对象意识，我们考虑了三种object-proposal模型。BING（Cheng等人，2014年）和Edge-Boxes（Zitnick & Dollár，2014年），这两个模型都是用Pascal-VOC的框训练的；还有一个无监督的物体提议方法（Vo等人，2019年）。BING（Cheng等人，2014）使用固定窗口大小内的梯度规范作为简单的特征，将其输入级联SVM框架以进行物体提议。这个方法是在PASCAL-VOC（Everingham等人，2010）训练集的2501张图像的真实边界框上训练的。请注意，BING不使用任何类别标签，而==只使用与标签无关的边界框信息==。BING的优点是性能极快（在300×300大小的图像上大于125 fps），能产生许多建议，并且对许多数据集有很好的通用性，正如Cheng等人（2014）所显示的和我们的经验结果中所看到的。这使得BING很适合作为一个物体提议算法。在我们的实验中，我们为每幅图像生成多达10个提议，并在这些提议中均匀地随机选择一个物体。其他object-proposal的细节见补充章节B。 我们在表6中的结果显示其他对象建议方法提供足够的质量，选择可能主要取决于方法的速度。在本文的其余部分，我们实验了四种裁剪类型。

**(a) 扩张的object-proposal(Obj-Obj+Dilate Crop):** 在空间上靠近物体的场景像素更有可能与物体有正相关关系。有了这种直觉，我们通过扩张BING建议来产生第二个视角。我们将框扩张10%或20%的图像大小，然后进行随机裁剪。改变 δ。使我们能够控制多少场景信息被纳入其中（在大多数情况下，10%的值很有效）。请注意，原始的和扩张的框后面都有一个随机的裁剪，确保第一个视图不会被琐碎地包括在第二个视图中。选择哪种裁剪作为 query 或 key 是任意的，物体和扩张的物体裁剪都可以作为 key 和 query。我们还考虑了下面列出的其他基线，这些基线是纳入场景信息的其他可信的方法。然而，正如第4节的结果所示，扩张的crop是表现最好的方法。

**(b) 场景-场景裁剪:** 我们在一幅图像中随机抽取两个裁剪，并将其视为正视图。这是MoCo-v2和其他SSL方法中使用的默认方法。根据我们在第2节的分析，这种方法在OpenImages等数据集上的表现很差。

**(c) 移位物体提议（Obj-Obj+移位裁剪）:** 与Obj-Obj+Dilate不同的是，在这里，第二个视图是在第一个框的预先指定的距离范围内随机选择的一个框（BING建议）。为了选择一个预先指定的距离，我们从几个范围中随机选择一个偏移值。80-100像素；100-120像素等。

**(d) 随机裁剪（Obj-Scene Crop）:** 第一个视图是BING的建议；而场景层面的常规随机裁剪是第二个视图。这种方法提供了物体和场景两个层面的信息，我们把它作为向模型添加上下文信息的基线。

**关于随机调整裁剪的下限的讨论:** 如上所述，对于Obj-Obj+Dilate和Obj-Obj+Shift裁剪，我们在BING对象建议或其移位或扩张的版本上使用随机裁剪，以生成最终视图。由于object-proposal本身是图像的一小部分（例如，在COCO中，物体裁剪通常覆盖图像的39%），使用通常的随机裁剪范围的默认下限值（通常为0.2）效果不佳，因为它导致图像的极小裁剪。因此，我们设置了下限，使其与通常的场景裁剪情况下的最小尺寸裁剪相匹配（$s_{min} = \dfrac{0.2}{平均BING建议尺寸}$）。

**对象和场景crop的不同投影头:** Chen等人（2020a）介绍的投影头是大多数SSL方法的一个重要组成部分。这是一个深度网络，它将编码器主干的表征映射到应用损失函数的低维空间。在Chen & He（2020）中，两个视图都使用了一个投影头。我们发现使用两个不同的投影头有利于提高性能：一个用于第一视角（BING裁剪），另一个用于第二视角的obj-scene、obj-obj+dilate和obj-obj+shift裁剪。对于场景-场景的裁剪，我们总是使用一个投影头。我们假设，==不同的投影网络专门用于特定物体的表示，或者用于包含场景信息的表示。==

除了上述对裁剪方法的改变外，我们对SSL流程的其他部分保持不变：其他的数据增强，如颜色偏移，以通常的方式应用，损失函数和训练制度也没有改变。因此，我们的方法对大多数SSL流程只包含非常简单的改变，只涉及几行代码。

### 4 结果
如第2节所述，我们创建了OpenImages数据集的一个子集，有212k张图片。此外，我们在ImageNet（Deng等人，2009）和MS-COCO（Lin等人，2014）上进行了预训练。ImageNet（有120万张训练图像）已被广泛使用，是用于SSL方法基准测试的标准数据集。MS-COCO有11.8万张训练图像和8.96万个标记的物体，每张图像大约有7个物体。对于MoCo-v2的预训练，我们严格遵循Chen等人（2020b）中描述的标准协议。我们从BING、Edge-Boxes或（Vo等人，2019）的无监督提议方法提供的10个物体提议中随机选择。我们所有的训练和评估都在ResNet-50（He等人，2015）上进行。

我们在分类（线性评估）、物体检测和语义分割上评估预训练的模型。对于VOC对象检测、COCO对象检测和COCO语义分割，我们严格遵循Detectron2（Wu等人，2019）中列出的通用协议。对于VOC物体检测，我们在VOC trDLnval07+12数据集上使用标准的1×标准协议对Faster-RCNN(C4-backbone)（Ren等人，2015）检测器进行评估。对于COCO-物体检测和语义分割，我们在MaskRCNN检测器（FPN-backbone）（He等人，2018）上进行微调，在COCO trDLn2017分割（118k图像）上使用标准的1×时间表，对COCO 5k val2017分割进行评估。我们与最先进的SSL方法进行比较，包括Self-EMD（Liu等人，2021a）、DetCon（Hénaff等人，2021）、BYOL（Richemond等人，2020）、DenseCL（Wang等人，2021）和默认的MoCo-v2（陈等人，2020b）。

表1和表3显示了OpenImages数据集的结果。我们对3种不同类型的crop进行了消融研究。我们可以看到，表现最好的Obj-Obj+Dilate裁剪比基线要好8.2mAP，将监督学习和MoCo-v2基线之间的差距缩小了近50%。Obj-Scene裁剪的表现也优于基线8.1mAP。Obj-Obj+Shift裁剪（54.1 mAP）的表现不如Obj-Obj+Dilate或Obj-Scene。我们认为这是因为第二个物体crop被选在离第一个物体crop一定的最小半径处（否则，作物之间的重叠太大，无法产生有意义的学习）。因此，第二个物体裁剪只包含物体的一部分。相反，一个扩张的物体裁剪有可能包含整个物体和更多的场景信息；因此，来自扩张的物体裁剪的表示包含来自物体和场景的互补信息。我们还在表3中显示了用 ground truth 边界框来指导物体裁剪的消融。这比使用BING裁剪的效果要好一些，这表明对于改进的表征来说，物体周围的紧密配合是没有必要的。
![image.png](https://s1.vika.cn/space/2023/02/08/02f5353af29647e5b51f7b1aa69ba712)

![image.png](https://s1.vika.cn/space/2023/02/08/7d77107c02534ff6bbcb52ab5f74e5b8)

表4显示了COCO的物体检测和语义分割的结果（通过对COCO trDLnval2017数据集的预训练和对COCO的微调）。我们训练MoCo-v2和BYOL模型。我们的MoCo-v2 Obj-Obj+Dilate裁剪优于MoCo-v2场景-场景基线和Obj-场景裁剪。对于语义分割，我们的表现优于MoCo-v2基线。我们提出的裁剪方法与预训练的SSL方法无关；我们通过将我们的方法添加到Dense-CL（Wang等人，2021）中，显示了最先进的结果。我们的表现也优于CAST模型（Selvaraju等人，2020），该模型也使用了基于显著性地图的本地化crop：我们的方法更简单，表现更好，约0.5mAP。表6（补充）显示了PASCAL-VOC上物体检测的结果。我们在COCO上进行预训练，然后在VOC上进行微调。Obj-Obj+Dilate的裁剪结果比MoCo-v2基线高出3.2 mAP；Obj-Scene的裁剪结果比MoCo-v2基线高出2.4 mAP，比BYOL基线高出2.5 mAP。在飞机、鸟类和汽车等标志性数据集上的改进结果可以在补充文件中找到（表10）。关于使用不同数量的提案的额外结果可以在补充文件（表8）中找到。

![image.png](https://s1.vika.cn/space/2023/02/08/fd578868693a4064a8447ad5cd37094e)

表5显示了通过对完整的OpenImages数据集（Kuznetsova等人，2020）（全部190万张图像）进行75个历时的预训练，对COCO的物体检测和语义分割以及VOC的物体检测的结果。我们表明，通过使用Obj-Obj+Dilate裁剪，物体检测和语义分割任务的性能都比基线有所提高。我们提出的扩张方法不仅适用于像COCO这样的小型多物体数据集，也适用于像OpenImages这样的数据集，并且在转移学习设置下表现良好。
![image.png](https://s1.vika.cn/space/2023/02/08/af9c0f448d924c1f8edaa1cbadfb2c29)

**在ImageNet上的结果。** 我们还显示了使用对象识别裁剪（表13补充）和MoCo-v2对ImageNet进行预训练的改进结果。对于VOC2007的物体检测，我们看到了1.0 mAP的改进；对于COCO的物体检测，我们看到了0.5 mAP的改进。因此，我们的方法是可以适应预训练数据集和SSL算法的。

**使用多个投影头。** 在OpenIm-ages分类中对每个视图使用不同的投影头，使我们在Obj-Obj+Dilate裁剪上获得了1.1 mAP的提升。在COCO上进行的预训练和在VOC数据集上进行的物体检测任务的微调给我们带来了0.4 mAP的提升。因此，使用多个投影头会带来持续的改进。

**扩张参数的变化。** 表9（补充）显示了改变扩张参数的效果。没有扩张就会变成Obj-Obj裁剪，而大的扩张参数则对应于Obj-Scene。对于COCO物体检测来说，在δ=0.1的适度扩张值下存在一个最佳点。

**计算成本：** BING为预训练增加的时间可以忽略不计。对于完整的OpenImages数据集，生成对象建议需要29分钟（一次性成本），对于COCO需要16分钟。与其说是预生成，不如说是在数据加载器流程中加入BING运算符，其开销微不足道（+0.1%）。

**对Obj-Scene和Obj-Obj+Shift裁剪的分析。** 如Tian等人（2019）所示，对比性自监督方法的性能是有 "甜蜜点 "的，它对应于视图之间的最佳MI量。我们对Obj-Scene裁剪方法进行了类似的分析，创建了重叠度逐渐增加的作物。图7（左）（补充）显示在重叠度为57.2%时，性能达到峰值。这一发现与（Tian等人，2019）一致。场景-场景裁剪的最佳重叠度较高，为65.2%。改变Obj-Obj+Shift作物的半径（图7（右）为OpenImages；图6（右）为COCO）显示了类似的结果。我们在D节中展示了对作物和感兴趣的物体之间重叠的额外分析。

### 5 相关工作
基于对比性和非对比性方法的自监督学习的最新进展，在各种领域、数据集和任务上取得了卓越的表现（He et al., 2019; Chen et al., 2020b;a; van den Oord et al, 2018；Tian等人，2019；Gidaris等人，2020；Misra & van der Maaten，2019；Tian等人，2020；Wu等人，2018；Grill等人，2020；Gidaris等人，2018；Larsson等人，2017；Noroozi 等人，2018；Pathak 等人，2016）。表现最好的方法都使用了相关的想法，将样本在表示空间中的 "视图" 拉到一起。此外，其中一些方法还使用负样本来增加 "推动 "因素，这被称为对比性自我监督学习。为了更好地理解这些方法的行为和局限性，理论和经验研究已经发表（Arora等人，2019；Xiao等人，2021；Purushwalkam & Gupta，2020；Tosh等人，2021；Wang & Isola，2020；Yang等人，2020；Chuang等人，2020；Liu等人，2021b；Kalantidis等，2020；Newell & Deng，2020；CDL等人，2020）。

一些论文观察到，上述默认的SSL方法（无论是否有对比性）在未经整理的数据集上表现不佳，如OpenImages（Kuznetsova等人，2020）。为了解决这个问题，最近的工作采用了不同的变通方法，如知识提炼（Tian等人，2021）、聚类（Goyal等人，2021）、本地化（Selvaraju等人，2020）、无监督语义分割掩码（Hénaff等人，2021）、像素级借口任务（Xie等人，2021）、实例本地化（Yang等人，2021）和局部对比性学习（Liu等人，2021a）。在表现最好的基于图像的SSL方法中，无论数据集、任务或架构如何，它们的共同点是依赖于强大的数据增强，如随机剪裁、高斯模糊、颜色抖动或旋转。这些增强创造了有意义的正视图，而在对比性SSL方法的情况下，数据集中的其他随机采样图像被用来创造负视图。SSL数据增强流程改编自监督学习文献（Cubuk等人，2018；Zoph等人，2020；Krizhevsky等人，2012；Simard等人，2003；DeVries & Taylor，2017；Cubuk等人，2018；张等人，2018；Cubuk等人，2019；吴等人，2019；云等人，2019；Lim等人，2019；Hataya等人，2019）。

与我们的物体裁剪工作最接近的工作是（Selvaraju等人，2020），它引入了一种技术，根据突出性地图在物体周围选择裁剪（Selvaraju等人，2016），与COCO数据集的随机裁剪基线相比，显示出良好的改进（见表4）。如我们的结果所示，Obj-Obj+Dilate裁剪的表现一直优于（Selvaraju等人，2020）（表4）。我们的方法也明显更简单，可以纳入现有的流程，不需要改变训练、架构或损失。Gansbeke等人（2021年）表明，受限的多重裁剪改善了SSL方法的性能：我们的方法可以被纳入他们的流程，以进一步提高性能。

### 6 结论
我们引入了对象感知裁剪，这是一种简单、快速和高效的数据增强，可以替代随机场景裁剪。我们进行了大量的实验，表明在一些数据集的分类、物体检测和语义分割的自监督预训练中，物体裁剪的性能比场景裁剪明显提高。该方法可以以无缝方式纳入大多数自监督学习流程。

