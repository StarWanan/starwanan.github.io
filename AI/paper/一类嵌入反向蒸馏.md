# Anomaly Detection via Reverse Distillation from One-Class Embedding

通过一类嵌入的反向蒸馏进行异常检测

## Abstract 

Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous stud- ies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective ==“reverse distillation” paradigm== accordingly. Instead of receiving raw images directly, the student network takes teacher model’s one-class embedding as input and targets to restore the teacher’s multi-scale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable ==one-class bottleneck embedding (OCBE) module== in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but abandons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach’s effectiveness and generalizability.

知识蒸馏（KD）在无监督异常检测（AD）的挑战性问题上取得了可喜的成果。 师生（T-S）模型中异常的表示差异为AD提供了必要的证据。然而，在以前的研究中使用相似或相同的架构来构建教师和学生模型阻碍了异常表示的多样性。 为了解决这个问题，我们提出了一种由教师编码器和学生解码器组成的新型 T-S 模型，并相应地引入了一种简单而有效的“逆向蒸馏”范式。学生网络不是直接接收原始图像，而是将教师模型的一类嵌入作为输入和目标，以恢复教师的多尺度表示。 本质上，本研究中的知识蒸馏从抽象的高级表示开始到低级特征。此外，我们在 T-S 模型中引入了可训练的一类瓶颈嵌入 (OCBE) 模块。 获得的紧凑嵌入有效地保留了正常模式的基本信息，但放弃了异常扰动。 对 AD 和一类新颖性检测基准的广泛实验表明，我们的方法超越了 SOTA 性能，证明了我们提出的方法的有效性和普遍性。

## 1. Introduction 

> 图1. MVTec[3]上的异常检测实例。多分辨率知识蒸馏（MKD）[33]采用了图2（a）中的传统KD架构。我们的反向蒸馏方法能够精确定位各种异常现象。
> <img src="https://s1.vika.cn/space/2022/06/21/4387e113c2b14509ae90d11a495b8c37" alt="image-20220621155304521" style="zoom:30%;" />

Anomaly detection (AD) refers to **identifying and localizing anomalies** with limited, even no, prior knowledge of abnormality. The wide applications of AD, such as indus- trial defect detection [3], medical out-of-distribution detection [50], and video surveillance [24], makes it a critical task as well as a spotlight. In the context of **unsupervised AD**, **no prior information** on anomalies is available. Instead, a set of normal samples is provided for reference. To tackle this problem, previous efforts attempt to construct various self-supervision tasks on those anomaly-free samples. These tasks include, but not limited to, sample reconstruction [2, 5, 11, 16, 26, 34, 38, 48], pseudo-outlier augmentation [23, 42, 46], knowledge distillation [4, 33, 39], etc.In this study, we tackle the problem of unsupervised anomaly detection from the knowledge distillation-based point of view. In **knowledge distillation (KD)** [6, 15], knowledge is transferred within a teacher-student (T-S) pair. In the context of unsupervised AD, since the student experiences only normal samples during training, it is likely to generate discrepant representations from the teacher when a query is anomalous. This hypothesis forms the basis of KD-based methods for anomaly detection. However, this hypothesis is not always true in practice due to 
(1) the identical or similar architectures of the teacher and student networks (i.e., non-distinguishing filters [33]) 
(2) the same data flow in the T-S model during knowledge transfer/distillation. 
Though the use of a smaller student network partially addresses this issue [33, 39], the weaker represen- tation capability of shallow architectures hinders the model from precisely detecting and localizing anomalies.

异常检测 (AD) 是指在对异常的先验知识有限甚至没有的情况下识别和定位异常。AD的广泛应用，如工业缺陷检测 [3]、医疗分布外检测 [50] 和视频监控 [24]，使其成为一项关键任务和聚光灯。 在无监督AD的背景下，没有关于异常的先验信息可用。 相反，提供了一组正常样本以供参考。 为了解决这个问题，以前的努力试图在那些无异常的样本上构建各种自我监督任务。 这些任务包括但不限于样本重建[2、5、11、16、26、34、38、48]、伪异常值增强[23、42、46]、知识蒸馏[4、33、39]等。在这项研究中，我们**基于知识蒸馏的角度解决了无监督异常检测的问题**。 在知识蒸馏 (KD) [6, 15] 中，知识在师生 (T-S) 对中转移。 在无监督 AD 的背景下，由于学生在训练期间只体验到正常样本，因此当查询异常时，它可能会从教师那里产生不一致的表示。 该假设构成了基于 KD 的异常检测方法的基础。 然而，这个假设在实践中并不总是正确的，因为 
(1)教师和学生网络的相同或相似架构（即非区分过滤器 [33]）
(2)在知识转移/蒸馏时T-S模型中的相同数据流。 
尽管使用较小的学生网络部分解决了这个问题 [33, 39]，但浅层架构较弱的表示能力阻碍了模型精确检测和定位异常。

> 较小的学生网络就不会和教师模型有相似的架构，更可能在面对异常时产生不一致表示。但是同时，较小的架构可能结果不那么可靠，导致面对任何数据时都有可能产生和教师网络不一致的表现



To holistically address the issue mentioned above, we propose a new paradigm of knowledge distillation, namely Reverse Distillation, for anomaly detection. We use sim- ple diagrams in Fig. 2 to highlight the systematic differ-ence between conventional knowledge distillation and the proposed reverse distillation. 
First, unlike the conventional knowledge distillation framework where both teacher and student adopt the encoder structure, the T-S model in our reverse distillation consists of heterogeneous architectures: a teacher encoder and a student decoder. 
Second, instead of directly feeding the raw data to the T-S model simultaneously, the student decoder takes the low-dimensional embedding as input, targeting to mimic the teacher's behavior by restoring the teacher model’s representations in different scales. 
From the regression perspective, our reverse distillation uses the student network to predict the representation of the teacher model. Therefore, "reverse" here indicates both the reverse shapes of teacher encoder and student decoder and the distinct knowledge distillation order where high-level representation is first distilled, followed by low-level features. It is noteworthy that our reverse distillation presents two significant advantages: 

i) Non-similaritystructure structure. In the proposed T-S model, one can consider the teacher encoder as a down-sampling filter and the stu- dent decoder as an up-sampling filter. The ”reverse structures” avoid the confusion caused by non-distinguishing filters [33] as we discussed above. 

ii) *Compactness embedding*. The low-dimensional embedding fed to the student decoder acts as an information bottleneck for normal pat- tern restoration. Let’s formulate anomaly features as pertur- bations on normal patterns. Then the compact embedding helps to prohibit the propagation of such unusual perturba- tions to the student model and thus boosts the T-S model’s representation discrepancy on anomalies. Notably, tradi- tional AE-based methods [5, 11, 16, 26] detect anomalies utilising pixel differences, whereas we perform discrimi- nation with dense descriptive features. Deep features as region-aware descriptors provide more effective discrimi- native information than per-pixel in images.

为了全面解决上述问题，我们提出了一种新的知识蒸馏范式，即==反向蒸馏==，用于异常检测。 我们使用图2中的简单图表来突出传统知识蒸馏和提出的逆向蒸馏之间的系统差异。 首先，与教师和学生都采用编码器结构的传统知识蒸馏框架不同，我们的逆向蒸馏中的 T-S 模型由**异构架构组成：教师编码器和学生解码器**。 其次，学生解码器不是直接将原始数据同时送入到 T-S 模型，而是将低维嵌入作为输入，旨在通过恢复教师模型在不同尺度上的表示来模仿教师的行为。
从回归的角度来看，我们的反向蒸馏使用学生网络来预测教师模型的表示。 因此，这里的“反向”表示教师编码器和学生解码器的**反向形状以及不同的知识蒸馏顺序**，即首先提炼高层次的表征，然后是低层次的特征。 值得注意的是，我们的逆向蒸馏具有两个显着优势：
i）非相似性结构。 在提出的 T-S 模型中，可以将教师编码器视为下采样滤波器，将学生解码器视为上采样滤波器。 正如我们上面讨论的，“反向结构”避免了由非区分过滤器[33]引起的混淆。

> 非区别性过滤器问题（non-distinguishing filter problem）

ii) 紧凑性嵌入。 馈送到学生解码器的低维嵌入充当了正常模式恢复的信息瓶颈。 让我们将异常特征表述为对正常模式的扰动。 然后紧凑嵌入有助于禁止这种不寻常的扰动传播到学生模型，从而提高 T-S 模型对异常的表示差异。 
值得注意的是，传统的基于 AE 的方法 [5、11、16、26] 利用像素差异检测异常，而我们使用密集的描述性特征进行区分。 作为区域感知描述符的深度特征比图像中的每个像素提供更有效的判别信息。

> 图2. (a)传统的KD框架[6, 33]和(b)我们的反向蒸馏范式中的T-S模型和数据流。
> <img src="https://s1.vika.cn/space/2022/06/21/4f0ca2610c5e49fdb5fa253df9b6f0ea" alt="image-20220621155403283" style="zoom:50%;" />



In addition, since the compactness of the bottleneck em- bedding is vital for anomaly detection (as discussed above), we introduce a one-class bottleneck embedding (OCBE) module to condense the feature codes further. Our OCBE module consists of a multi-scale feature fusion (MFF) block and one-class embedding (OCE) block, both jointly optimized with the student decoder. Notably, the former aggregates low-level and high-level features to construct a rich embedding for normal pattern reconstruction. The latter targets to retain essential information favorable for the student to decode out the teacher’s response.
We perform extensive experiments on public bench- marks. The experimental results indicate that our reverse distillation paradigm achieves comparable performance with prior arts. The proposed OCBE module further improves the performance to a new state-of-the-art (SOTA) record. Our main contributions are summarized as follows:  

- We introduce a simple, yet effective Reverse Distillation paradigm for anomaly detection. The encoder- decoder structure and reverse knowledge distillation strategy holistically address the non-distinguishing filter problem in conventional KD models, boosting the T-S model’s discrimination capability on anomalies.

- We propose a one-class bottleneck embedding mod- ule to project the teacher’s high-dimensional features to a compact one-class embedding space. This innovation facilitates retaining rich yet compact codes for anomaly-free representation restoration at the student.
- We perform extensive experiments and show that our approach achieves new SOTA performance.

此外，由于瓶颈嵌入的紧凑性对于异常检测至关重要（如上所述），我们引入了==一类瓶颈嵌入（OCBE）模块==来进一步压缩特征代码。 我们的 OCBE 模块由**多尺度特征融合 (MFF) 块**和**一类嵌入 (OCE) 块**组成，两者都与学生解码器联合优化。 值得注意的是，前者聚合了低级和高级特征，为正常模式的重建构建了丰富的嵌入。后者的目标是保留有利于学生的基本信息，去解码教师的回应。
我们在公共基准上进行了广泛的实验。 实验结果表明，我们的反向蒸馏范式实现了与现有技术相当的性能。 所提出的 OCBE 模块进一步将性能提高到新的最先进 (SOTA) 记录。 我们的主要贡献总结如下： 

- 我们为异常检测引入了一种简单而有效的逆向蒸馏范式。 编码器-解码器结构和反向知识蒸馏策略整体解决了传统 KD 模型中的非区分过滤器问题，提高了 T-S 模型对异常的判别能力。

- 我们提出了一类瓶颈嵌入模块，将教师的高维特征投影到紧凑的一类嵌入空间。 这项创新有助于保留丰富而紧凑的代码，以便在学生处进行无异常表示恢复。

- 我们进行了广泛的实验并表明我们的方法实现了新的 SOTA 性能。

## 2. Related Work

This section briefly reviews previous efforts on unsupervised anomaly detection. We will highlight the similarity and difference between the proposed method and prior arts.

本节简要回顾了以前在**无监督异常检测**方面的努力。 我们将强调所提出的方法与现有技术之间的相似之处和不同之处。



Classical anomaly detection methods focus on defining a compact closed one-class distribution using normal sup- port vectors. The pioneer studies include one-class support vector machine (OC-SVM) [35] and support vector data description (SVDD) [36]. To cope with high-dimensional data, DeepSVDD [31] and PatchSVDD [43] estimate data representations through deep networks.

经典的异常检测方法侧重于使用正态支持向量定义紧凑的封闭一类分布。 先驱研究包括一类支持向量机（OC-SVM）[35]和支持向量数据描述（SVDD）[36]。 为了处理高维数据，DeepSVDD [31] 和 PatchSVDD [43] 通过深度网络估计数据表示。



Another unsupervised AD prototype is the use of generative models, such as AutoEncoder (AE) [19] and Generative Adversarial Nets (GAN) [12], for sample reconstruc- tion. These methods rely on the hypothesis that generative models trained on normal samples only can successfully reconstruct anomaly-free regions, but fail for anoma- lous regions [2, 5, 34]. However, recent studies show that deep models generalize so well that even anomalous re- gions can be well-restored [46]. To address this issue, memory mechanism [11, 16, 26] , image masking strategy [42, 46] and pseudo-anomaly [28, 45] are incorporated in reconstruction-based methods. 
However, these meth- ods still lack a strong discriminating ability for real-world anomaly detection [3, 5]. Recently, Metaformer (MF) [40] proposes the use of meta-learning [9] to bridge model adap- tation and reconstruction gap for reconstruction-based ap- proaches. Notably, the proposed reverse knowledge distillation also adopts the encoder-decoder architecture, but it differs from construction-based methods in two-folds. First, the encoder in a generative model is jointly trained with the decoder, while our reverse distillation freezes a pre-trained model as the teacher. Second, instead of pixel-level recon- struction error, it performs anomaly detection on the semantic feature space.

另一个无监督的 AD 原型是使用生成模型，例如自动编码器 (AE) [19] 和生成对抗网络 (GAN) [12]，用于样本重建。 这些方法依赖于这样一个假设，即在正常样本上训练的生成模型只能成功地重建无异常区域，但对于异常区域则失败 [2, 5, 34]。 然而，最近的研究表明，深度模型的泛化能力非常好，即使是异常区域也可以很好地恢复 [46]。 为了解决这个问题，记忆机制[11、16、26]、图像掩蔽策略[42、46]和伪异常[28、45]被纳入基于重建的方法中。 
然而，这些方法对于现实世界的异常检测仍然缺乏很强的辨别能力[3, 5]。 最近，Metaformer (MF) [40] 提出使用元学习 [9] 来弥合基于重建的方法的模型适应和重建差距。 值得注意的是，所提出的反向知识蒸馏也采用了编码器-解码器架构，但它与基于构造的方法有两方面的不同。 首先，生成模型中的编码器与解码器联合训练，而我们的逆向蒸馏将预先训练的模型冻结为教师。 其次，它不是像素级的重建错误，而是对语义特征空间进行异常检测。



Data augmentation strategy is also widely used. By adding pseudo anomalies in the provided anomaly-free samples, the unsupervised task is converted to a supervised learning task [23, 42, 46]. However, these approaches are prone to bias towards pseudo outliers and fail to detect a large variety of anomaly types. For example, CutPaste [23] generates pseudo outliers by adding small patches onto nor- mal images and trains a model to detect these anomalous regions. Since the model focuses on detecting local fea- tures such as edge discontinuity and texture perturbations, it fails to detect and localize large defects and global struc- tural anomalies as shown in Fig. 6.

数据增强策略也被广泛使用。 通过在提供的无异常样本中添加伪异常，将无监督任务转换为监督学习任务 [23,42,46]。 然而，这些方法容易偏向伪异常值，并且无法检测到多种异常类型。 例如，CutPaste [23] 通过在正常图像上添加小块来生成伪异常值，并训练模型来检测这些异常区域。 由于该模型侧重于检测局部特征，例如边缘不连续性和纹理扰动，因此无法检测和定位大缺陷和全局结构异常，如图 6 所示。

> 图6. 从上到下的异常情况。"金属螺母的 "翻转"，晶体管的 "错位 "和榛子的 "裂缝"。正常的样本被提供作为参考。
> <img src="https://s1.vika.cn/space/2022/06/21/ccad4d9249f5457bae002ca6cbf083f9" alt="image-20220621162602942" style="zoom:50%;" />



Recently, networks pre-trained on the large dataset are proven to be capable of extracting discriminative features for anomaly detection [7,8,23,25,29,30]. With a pre-trained model, memorizing its anomaly-free features helps to iden- tify anomalous samples [7, 29]. The studies in [8, 30] show that using the Mahalanobis distance to measure the simi- larity between anomalies and anomaly-free features leads to accurate anomaly detection. Since these methods re- quire memorizing all features from training samples, they are computationally expensive.

最近，在大型数据集上预训练的网络被证明能够提取用于异常检测的判别特征 [7,8,23,25,29,30]。 使用预训练模型，记住其无异常特征有助于识别异常样本 [7, 29]。  [8, 30] 中的研究表明，使用马氏距离来测量异常和无异常特征之间的相似性可以实现准确的异常检测。 由于这些方法需要记住训练样本的所有特征，因此它们的计算成本很高。



Knowledge distillation from pre-trained models is an- other potential solution to anomaly detection. In the con- text of unsupervised AD, since the student model is ex- posed to anomaly-free samples in knowledge distillation, the T-S model is expected to generate discrepant features on anomalies in inference [4,33,39]. To further increase the discrimnating capability of the T-S model on various types of abnormalities, different strategies are introduced. For in- stance, in order to capture multi-scale anomaly, US [4] en- sembles several models trained on normal data at different scales, and MKD [33] propose to use multi-level features alignment. It should be noted that though the proposed method is also based on knowledge distillation, our reverse distillation is the first to adopt an encoder and a decoder to construct the T-S model. The heterogeneity of the teacher and student networks and reverse data flow in knowledge distillation distinguishes our method from prior arts.

来自预训练模型的知识蒸馏是异常检测的另一个潜在解决方案。 在无监督 AD 的背景下，由于学生模型在知识蒸馏中暴露于无异常样本，因此 T-S 模型预计会在推理异常上产生差异特征 [4,33,39]。 为了进一步提高 T-S 模型对各类异常的判别能力，引入了不同的策略。 例如，为了捕获多尺度异常，US [4] 集成了几个在不同尺度的正常数据上训练的模型，MKD [33] 建议使用多级特征对齐。 需要注意的是，虽然所提出的方法也是基于知识蒸馏的，但我们的逆向蒸馏是第一个采用编码器和解码器来构建 T-S 模型的方法。 教师和学生网络的异质性以及知识蒸馏中的反向数据流将我们的方法与现有技术区分开来。

## 3. Our Approach

**Problem formulation:** Let It = {It1, ..., It n} be a set of available anomaly-free images and Iq = {Iq1 , ..., Iq m}be a query set containing both normal and abnormal samples. The goal is to train a model to recognize and localize anomalies in the query set.  In the anomaly detection setting, normal samples in both It and Iq follow the same distribution. Out-of-distribution samples are considered anomalies.

**问题的提出：**让 $\mathcal{I}^t = \{I^t_1, ..., I_t^n\}$ 是一个可用的无异常图像集，$\mathcal{I}^q = \{I^q_1, ..., I_q^n\}$ 是一个包含正常和异常样本的查询集。我们的目标是训练一个模型来识别和定位查询集中的异常情况。 在异常检测设置中，It和Iq中的正常样本都遵循相同的分布。分布外的样本被认为是异常的。



**System overview:** Fig. 3 depicts the proposed reserve distillation framework for anomaly detection. Our reverse distillation framework consists of three modules: a fixed pre-trained teacher encoder E, a trainable one-class bottleneck embedding module, and a student decoder D. 

Given an input sample I ∈ It, the teacher E extracts multiscale representations. We propose to train a student D to restore the features from the bottleneck embedding. During testing/inference, the representation extracted by the teacher E can capture abnormal, out-of-distribution features in anomalous samples. However, the student decoderD fails to reconstruct these anomalous features from the corresponding embedding. The low similarity of anomalous representations in the proposed T-S model indicates a high abnormality score. We argue that the heterogeneous encoder and decoder structures and reverse knowledge distillation order contribute a lot to the discrepant representations of anomalies. In addition, the trainable OCBE module further condenses the multi-scale patterns into an extreme low-dimensional space for downstream normal representation reconstruction. This further improves feature discrepancy on anomalies in our T-S model, as abnormal representations generated by the teacher model are likely to be abandoned by OCBE. 

In the rest of this section, we first specify the reverse distillation paradigm. Then, we elaborate on the OCBE module. Finally, we describe anomaly detection and localization using reserve distillation.

**系统概述：**图3描述了提出的用于异常检测的反向蒸馏框架。我们的反向蒸馏框架由三个模块组成：==固定的预训练的教师编码器$E$==、==可训练的单类瓶颈嵌入模块==,和==学生解码器$D$==.

给定输入样本 $I\in \mathcal{I}^t$ ，教师E提取多尺度表示。我们建议训练一名学生D从瓶颈嵌入中恢复特征。在测试/推理过程中，老师E提取的表征可以捕获异常样本中的异常、外分布特征。然而，学生解码器D不能从相应的嵌入中重建这些异常特征。
在所提出的T-S模型中，异常表征的相似度较低，表明异常得分较高。我们认为，编码和译码结构的异构性以及反向知识蒸馏顺序对异常的不一致表示有很大影响。此外，可训练的OCBE模块进一步将多尺度模式压缩到极低维空间，用于下游正常的表示重建。这进一步改善了T-S模型中异常的特征差异，因为教师模型生成的异常表示可能会被OCBE丢弃。

在本节的其余部分中，我们首先指定反向蒸馏范例。然后，我们详细介绍了OCBE模块。最后，我们描述了利用储备蒸馏进行异常检测和定位。

> 图3. 我们用于异常检测和定位的反向蒸馏框架概述。(a) 我们的模型由一个预先训练好的教师编码器E、一个可训练的单类瓶颈嵌入模块（OCBE）和一个学生解码器D组成。我们使用多尺度特征融合（MFF）块来集合E的低级和高级特征，并通过单类嵌入（OCE）块将它们映射到一个紧凑的代码上。在训练过程中，学生D通过最小化相似性损失L来学习模仿E的行为。 在推理过程中，E真实地提取特征，而D则输出无异常的特征。E和D对应位置的特征向量之间的低相似度意味着有异常情况。(c) 最终的预测是由多尺度相似度图M的累积计算出来的。
> ![image-20220621163311245](https://s1.vika.cn/space/2022/06/21/9010625f14664e93b1998731925fbff7)



### 3.1. Reverse Distillation

In conventional KD, the student network adopts a similar or identical neural network to the teacher model, accepts raw data/images as input, and targets to match its feature activations to the teacher's [4, 33]. In the context of one-class distillation for unsupervised AD, the student model is expected to generate highly different representations from the teacher when the queries are anomalous samples [11, 26]. However, the activation discrepancy on anomalies vanishes sometimes, leading to anomaly detection failure. 

We argue that this issue is attributed to the similar architectures of the teacher and student nets and the same data flow during T-S knowledge transfer. To improve the T-S model's representation diversity on unknown, out-of-distribution samples, we propose a novel reserves distillation paradigm, where the T-S model adopts the encoder-decoder architecture and knowledge is distilled from teacher's deep layers to its early layers, i.e., high-level, semantic knowledge being transferred to the student first. To further facilitate the oneclass distillation, we designed a trainable OCEB module to connect the teacher and student models (Sec. 3.2).

在传统的KD中，学生网络采用与教师模型相似或相同的神经网络，接受原始数据/图像作为输入，并以匹配其特征激活与教师的[4，33]为目标。在无监督AD的一类蒸馏的背景下，当查询是异常样本时，学生模型预计会产生来自教师的高度不同的表示[11，26]。然而，异常的激活差异有时会消失，导致异常检测失败。

我们认为，这一问题归因于教师网和学生网的相似结构以及T-S知识传递过程中的相同数据流。为了提高T-S模型在未知、非分布样本上的表示多样性，我们提出了一种新的保留蒸馏范式，该模型采用编码器-解码器结构，将知识从教师的深层提取到其早期层，即首先将高层的语义知识传递给学生。为了进一步促进一类蒸馏，我们设计了一个可训练的OCEB模块来连接教师和学生模型(SEC。3.2)。



In the reverse distillation paradigm, the teacher encoder E aims to extract comprehensive representations. We follow previous work and use a pre-trained encoder on ImageNet [21] as our backbone E. To avoid the T-S model converging to trivial solutions, all parameters of teacher Eare frozen during knowledge distillation. We show in the ablation study that both ResNet [14] and WideResNet [44] are good candidates, as they are capable of extracting rich features from images [4, 8, 23, 29].

在反向蒸馏范式中，**教师编码者E的目标是提取综合表征**。我们在前人工作的基础上，使用ImageNet[21]上预先训练的编码器作为我们的主干E。为了避免T-S模型收敛到琐碎的解，教师E的所有参数在知识提炼过程中被冻结。我们在消融研究中表明，ResNet[14]和WideResNet[44]都是很好的候选者，因为它们能够从图像[4，8，23，29]中提取丰富的特征

> trivial solution，琐碎的解。是指不紧凑的嵌入特征吗？



To match the intermediate representations of E, the architecture of student decoder D is symmetrical but reversed compared to E. The reverse design facilitates eliminating the response of the student network to abnormalities, while the symmetry allows it to have the same representation dimension as the teacher network. For instance, when we take ResNet as the teacher model, the student D consists of several residual-like decoding blocks for mirror symmetry. Specifically, the downsampling in ResNet is realized by a convolutional layer with a kernel size of 1 and a stride of 2 [14]. The corresponding decoding block in the studentD adopts deconvolutional layers [47] with a kernel size of 2 and a stride of 2. More details on the student decoder design are given in Supplementary Material.

为了匹配E的中间表示，学生解码器D的体系结构与E相比是对称但反向的。**反向设计有助于消除学生网络对异常的响应**，而**对称性允许其具有与教师网络相同的表示维度**。例如，当我们使用ResNet作为教师模型时，学生D由几个镜像对称的类似残差的解码块组成。具体地，ResNet中的下采样由核大小为1、步长为2的卷积层来实现[14]。学生D中相应的译码块采用核大小为2、步长为2的反卷积层[47]。关于学生解码器设计的更多细节在补充资料中给出



In our reverse distillation, the student decoder D targets to mimic the behavior of the teacher encoder E during training. In this work, we explore multi-scale feature-based distillation for anomaly detection. The motivation behind this is that shallow layers of a neural network extract local descriptors for low-level information (e.g., color, edge, texture, etc.), while deep layers have wider receptive fields, capable of characterizing regional/global semantic and structural information. That is, low similarity of low- and highlevel features in the T-S model suggests local abnormalities and regional/global structural outliers, respectively.

在我们的反向蒸馏中，学生解码器D的目标是模仿教师编码器E在训练期间的行为。在这项工作中，我们探索了基于多尺度特征的蒸馏异常检测方法。这背后的动机是，神经网络的浅层提取低级信息(例如，颜色、边缘、纹理等)的局部描述符，而深层具有更广泛的接受域，能够表征区域/全局语义和结构信息。也就是说，T-S模型中低层特征和高层特征的低相似性分别表明局部异常和区域/全局结构离群值。



Mathematically, let φ indicates the projection from raw data I to the one-class bottleneck embedding space, the paired activation correspondence in our T-S model is {f k E =Ek(I), f k D = Dk(φ)}, where Ek and Dk represent thekth encoding and decoding block in the teacher and student model, respectively. f k E , f k D ∈ RCk ×Hk ×Wk , where Ck, Hkand Wk denote the number of channels, height and width of the kth layer activation tensor. For knowledge transfer in the T-S model, the cosine similarity is taken as the KD loss, as it is more precisely to capture the relation in both highand low-dimensional information [37, 49]. Specifically, for feature tensors f k E and f k D , we calculate their vector-wise cosine similarity loss along the channel axis and obtain a 2-D anomaly map M k ∈ RHk ×Wk :

在数学上，假设 $\phi$ 表示从原始数据 $I$ 到一类瓶颈嵌入空间的投影，T-S模型中的成对激活对应为 ${f^k_E=E^k(I)，f^k_D=D^k(φ)}$ ，其中 E^k^ 和 D^k^ 分别表示教师和学生模型中的第k个编解码块。$f^k_E，f^k_D \in \R^{C_k×H_K×W_k}$ ，其中C~k~、H~k~ 和 W~k~表示通道数、第k层激活张量的高度和宽度。

对于T-S模型中的知识转移，余弦相似度被视为KD损失，因为它更准确地捕捉了高维和低维信息中的关系[37，49]。具体地说，对于特征张量 $f^k_E，f^k_D$ ，我们计算了它们沿通道轴的矢量余弦相似损失，并得到了二维异常图 $M^k\in \R{H_k×W_k}$：
$$
M^k(h,w) = 1 - \frac{f_E^k(h,w) \cdot f_D^k(h,w)}{||f_E^k(h,w)||\ ||f_D^k(h,w)||}
$$


A large value in $M^k$ indicates high anomaly in that location. Considering the multi-scale knowledge distillation, the scalar loss function for student's optimization is obtained by accumulating multi-scale anomaly maps:

Mk值越大，表明该位置异常程度越高。考虑多尺度知识蒸馏，通过累加多尺度异常图得到学生优化的标量损失函数:
$$
\mathcal{L}_\mathcal{KD} = \sum\limits^K_{k=1} \Bigg\{ \frac{1}{H_kW_k} \sum\limits_{h=1}^{H_k}\sum\limits_{w=1}^{W_k}M^k(h,w)  \Bigg\}
$$
where K indicates the number of feature layers used in the experiment.
其中，K表示实验中使用的特征层数。

### 3.2. One-Class Bottleneck Embedding

Since the student model D attempts to restore representations of a teacher model E in our reverse knowledge distillation paradigm, one can directly feed the activation output of the last encoding block in backbone to D. However, this naive connection has two shortfalls. First, the teacher model in KD usually has a high capacity. Though the high-capacity model helps extract rich features, the obtained high-dimensional descriptors likely have a considerable redundancy. The high freedom and redundancy of representations are harmful to the student model to decode the essential anomaly-free features. Second, the activation of the last encoder block in backbone usually characterizes semantic and structural information of the input data. Due to the reverse order of knowledge distillation, directly feeding this high-level representation to the student decoder set a challenge for low-level features reconstruction. Previous efforts on data reconstruction usually introduce skip paths to connect the encoder and decoder. However, this approach doesn't work in knowledge distillation, as the skip paths leak anomaly information to the student during inference.

由于学生模型D试图在我们的反向知识蒸馏范例中恢复教师模型E的表示，所以可以直接将骨干中最后一个编码块的激活输出反馈给D。然而，这种天真的连接有两个缺陷。

- 首先，KD中的教师模式通常具有较高的能力。虽然大容量模型有助于提取丰富的特征，但获得的高维描述符可能具有相当大的**冗余度**。表示的高度自由度和冗余性不利于学生模型解码本质上的无异常特征。
- 第二，主干中最后一个编码块的激活通常表征了输入数据的语义和结构信息。由于知识蒸馏的顺序相反，直接将这些高层表示反馈给学生解码者，**给低层特征重构带来了挑战**。以往的数据重建工作通常引入跳连来连接编码器和解码器。然而，这种方法在知识提炼中不起作用，因为跳跃路径在推理过程中会向学生泄露异常信息。



To tackle the first shortfall above in one-class distillation, we introduce a trainable one-class embedding block to project the teacher model's high-dimensional representations into a low-dimensional space. Let's formulate anomaly features as perturbations on normal patterns. Then the compact embedding acts as an information bottleneck and helps to prohibit the propagation of unusual perturbations to the student model, therefore boosting the T-S model's representation discrepancy on anomalies. In this study, we adopt the 4th residule block of ResNet [14] as the one-class embedding block.

为了解决单级蒸馏的第一个缺陷，我们引入了一种可训练的单级嵌入块来将教师模型的高维表示投影到低维空间。让我们**将异常特征表示为对正常模式的扰动**。紧凑嵌入成为信息瓶颈，有助于阻止异常扰动传播到学生模型，从而提高T-S模型对异常的表示差异。在本研究中，我们采用ResNet[14]的第四个剩余块作为一类嵌入块。



To address the problem on low-level feature restoration at decoder D, the MFF block concatenates multi-scale representations before one-class embedding. To achieve representation alignment in feature concatenation, we downsample the shallow features through one or more 3 × 3 convolutional layers with stride of 2, followed by batch normalization and ReLU activation function. Then a 1×1 convolutional layer with stride of 1 and a batch normalization with relu activation are exploited for a rich yet compact feature.

为了解决解码器D的低层特征恢复问题，MFF块在一类嵌入之前将多个尺度表示连接在一起。为了实现特征拼接中的表示对齐，我们通过一个或多个步长为2的3×3卷积层对浅层特征进行下采样，然后进行批归一化和RELU激活函数。然后利用步长为1的1×1卷积层和具有RELU激活的批归一化来获得丰富而紧凑的特征。



We depict the OCBE module in Fig. 4, where MFF aggregates low- and high-level features to construct a rich embedding for normal pattern reconstruction and OCE targets to retain essential information favorable for the student to decode out the teacher's response. The convolutional layers in grey and ResBlock in green in Fig. 4 are trainable and optimized jointly with the student model D during knowledge distillation on normal samples.

我们描述了图4中的OCBE模块，其中MFF聚集了低层和高层特征来构建一个丰富的嵌入，用于正常模式重建和OCE目标，以保留有利于学生解码教师反应的基本信息。在正常样本的知识蒸馏过程中，图4中灰色的卷积层和绿色的ResBlock是可训练的，并与学生模型D一起优化。

> 图4.我们的单类瓶颈嵌入模块由可训练的MFF和OCE块组成。MFF对齐了Teacher E的多尺度特征，而OCE将获得的丰富特征浓缩到一个紧凑的瓶颈代码φ。
> <img src="https://s1.vika.cn/space/2022/06/23/dbdbbb969e50484899c88c36e962cf20" alt="image-20220623002851797" style="zoom:50%;" />



### 3.3 Anomaly Scoring

At the inference stage, We first consider the measurement of pixel-level anomaly score for anomaly localization (AL). When a query sample is anomalous, the teacher model is capable of reflecting abnormality in its features. However, the student model is likely to fail in abnormal feature restoration, since the student decoder only learns to restore anomaly-free representations from the compact oneclass embedding in knowledge distillation. In other words, the student D generates discrepant representations from the teacher when the query is anomalous. Following Eq. (1), we obtain a set of anomaly maps from T-S representation pairs, where the value in a map Mk reflects the point-wise anomaly of the kth feature tensors. To localize anomalies in a query image, we up-samples M k to image size. LetΨ denotes the bilinear up-sampling operation used in this study. Then a precise score map SIq is formulated as the pixel-wise accumulation of all anomaly maps:

在推理阶段，我们首先考虑了用于异常定位(AL)的像素级异常得分的测量。当查询样本异常时，教师模型能够反映其特征中的异常。然而，学生模型很可能在异常特征恢复中失败，因为学生解码器只学习从嵌入知识精馏的紧凑类中恢复无异常表示。换句话说，当查询异常时，学生D生成来自教师的不一致表示。遵循公式：(1)，我们从T-S表示对得到了一组异常映射，其中映射Mk中的值反映了第k个特征张量的逐点异常。为了定位查询图像中的异常，我们将Mk向上采样到图像大小。设Ψ表示本研究中使用的双线性上采样操作。然后，精确的得分图SIQ被表示为所有异常图的像素累加：
$$
S_{AL} = \sum \limits^L_{i=1} \Psi(M^i)
$$
In order to remove the noises in the score map, we smoothSAL by a Gaussian filter.

为了去除分数图中的噪声，我们用高斯滤波对SAL进行平滑。



For anomaly detection, averaging all values in a score map SAL is unfair for samples with small anomalous regions. The most responsive point exists for any size of anomalous region. Hence, we define the maximum value in SAL as sample-level anomaly score SAD . The intuition is that no significant response exists in their anomaly score map for normal samples.

对于异常检测，对于具有小异常区域的样本，平均得分图SAL中的所有值是不公平的。对于任何大小的异常区域，都存在最大响应点。因此，我们将SAL中的最大值定义为样本级异常得分SAD。直觉是，在他们对正常样本的异常评分图中没有明显的反应。



