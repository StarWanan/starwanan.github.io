## 第一章 绪论

概念(选择/判断)

- 机器学习【7】: 构建映射函数

- 表示学习【8】: 从数据中学习好的表示

- 深度学习【11】: 构建有深度的模型, 自动学习好的特征表示

- 神经网络【13】

## 第二章 ==机器学习概述==

2.2 机器学习的==三个基本要素==【26】

- **模型**【26】
- **学习准则**
  - 损失函数【28】：01，平方，交叉熵，Hinge
  - 风险最小化【29】：经验，结构
- **优化算法**【31】
  - 学习率
  - 梯度下降
  - 提前停止: 验证集上的错误率不在下降

线性回归(要会)【34】

- 参数学习: 结构风险最小化
- 期望或者经验风险最小化 和 结构风险最小化之间的区别

==偏差方差分解==(要知道)【39】[详解](https://blog.csdn.net/qq_32742009/article/details/82142119)

评价指标(要知道)【46】

## 第三章 线性模型(1,2,3,4重要)

3.1 线性判别函数和决策边界【56】

- 二分类 / 多分类

3.2 Logistic【59】

3.3 Softmax. 多个类 (不用证明)【61】

3.4 感知器【64】

3.6 损失函数的对比【75】

## 第四章 前馈神经网络

**激活函数以及它们的函数图像**

1. 神经元
   - Sigmoid(83)
   - ReLu(86)
   - Maxout(89)
2. 网络结构(概念)【90】 （前馈，记忆，图）
3. 前馈神经网络
   1. ==前馈网络的拓扑结构==。各种符号的含义【92】
   2. 参数学习(95)
4. **反向传播算法**【96】
   - 给定一个 全连接/CNN/RNN 具体的网络, 会算偏导. 
   - **求固定结构的梯度**

## 第五章 ==卷积==神经网络 ⭐

==卷积的计算（卷积，激活，pooling）==

卷积的定义【110】

互相关【112】

变种【113】

用卷积来代替全连接【115】

卷积层【116】

汇聚层【118】

参数学习, 反向传播【120】

**典型的神经网络**(一定要知道)【121】

LeNet-5；AlixNet；Inception网络；ResNet残差网络(避免梯度消失问题)

**空洞卷积**【129】

- 为什么用来做卷积,不用做全连接?目的是什么? 处理图像时【109】

## 第六章 ==循环==神经网络

6.1 加了自回归模型的循环神经网络【134】

6.2 一个简单神经网络循环【135】

- **基本结构**

6.3 模式【138】**能干什么事情，以及干这些事情的时候用了什么网络**

- 序列到类别：分类
- 同步的序列到序列：序列标注 （词性标注）
- 异步的序列到序列：机器翻译

6.4 参数学习: 反向传播【140】

6.5 怎么解决长程依赖的问题? 怎么解决数据量过大的问题? 方法【144】

- 长程依赖导致
  - 梯度爆炸：循环神经网络进行权重衰减或梯度截断进行避免
  - 梯度消失：优化技巧或改变模型。 $h_t = h_{t-1} + g(x_t,h_{t-1};\theta)$ $h_t$ 和 $h_{t-1}$ 之间既有线性也有非线性关系。但改进仍然存在两个问题：
    - 梯度爆炸
    - 记忆容量问题 

==LSTM== 或 ==GRU== 

## 第七章 ==网络优化与正则化==（7.2 - 7.7 重要, 技巧, 仔细看）

7.1 网络结构多样性: 高维变量的非凸优化【158】

7.2 优化的算法（优化的改善方法. 哪些方法, 起什么作用）【160】

 - **学习率的调整**（衰减、预热、周期调整、AdaGrad、RMSprop、AdaData）学习率的方法: (原理, 好处坏处, 区别不同)
 - **梯度$g_t$**（动量、Nesterov加速梯度、Adam）

7.3 参数初始化【172】

7.4 数据预处理【177】

7.5 逐层归一化【179】

7.6 ==超参数优化==【183】

- **核心：将数据分成 —— 训练集、验证集、独立测试集。**
- **通过训练集、验证集进行超参数优化**

7.7 **网络正则化**（L1，L2，数据增强，提前停止，丢弃，集成）【187】

学习率什么情况下 可以 / 不能 跳出局部极小值

- 越稳定越可能靠近最小值
- 越不稳定越有可能冲出鞍点

## 第八章 注意力机制

QKV三个模型【204】

注意力机制解决什么问题？

- 避免的长程依赖问题，仿真的人类的注意力模型 ，减少，消除一些不必要的数值。

## 其它一些问题

<img src="C:\Users\DreamBoat\AppData\Roaming\Typora\typora-user-images\image-20220105093650752.png" alt="image-20220105093650752" style="zoom:30%;" />

神经网络参数初始化方法：预训练，随机初始化（不能全0）
分类问题常常使用交叉熵而不是欧氏距离（分类问题不连续，不能直接求距离）
dropout: 随即丢弃节点, 再进行反向传播。 降低计算量, 提高稳定性
改进网络：换激活函数、池化、归一化
1*1卷积出现时，在大多数情况下它作用是升/降特征的维度，这里的维度指的是通道数（厚度），而不改变图片的宽和高



参数更新的基本公式：$\theta_t = \theta_{t-1} - \alpha  g_t$  等价于 $\Delta \theta_t = - \alpha g_t$

修正 $\alpha$

1. AdaGrad 梯度累计 $G_t$

   $G_t = \sum_{\tau = 1}^{t} g_{\tau} * g_{\tau}$

   $\Delta \theta_t = - \frac{\alpha}{\sqrt{G_t + \epsilon}} g_t$

   这里新的学习率是单调下降的，而且是自适应调整 
   $\epsilon$ 是为了避免开始累计的时候是0 
   $\sqrt{}$ 是将点乘的平方开方, 使量纲一致

2. RMSprop    Ginton提出

   认为之前的梯度影响更小, 越远时刻梯度权重越小 【$G_0 = 0$】

$$
\begin{aligned}
G_t &= \beta G_{t-1} + (1-\beta)g_t * g_t \\
&= \beta [\beta G_{t-2}+(1-\beta)g_{t-1}*g_{t-1}] + (1-\beta)g_t * g_t \\ 
&= \beta^2G_{t-2} + \beta(1-\beta)g_{t-1}*g_{t-1} + (1-\beta)g_t * g_t \\ 
&= ...... \\
&= \beta^tG_0 + \beta^{t-1}(1-\beta)g_1*g_1 + ... + \beta^0(1-\beta)g_t * g_t \\
&= \beta^{t-1}(1-\beta)g_1*g_1 + \beta^{t-2}(1-\beta)g_2*g_2 + ... + \beta^0(1-\beta)g_t*g_t \\
&= (1-\beta)[\beta^{t-1}g_1*g_1 + \beta^{t-2}g_2*g_2 + ... + \beta^0g_t*g_t]\\
&= (1-\beta)\sum_{\tau=1}^t \beta^{t-\tau} g_{\tau} * g_{\tau}
\end{aligned}
$$

   $\Delta \theta_t = - \frac{\alpha}{\sqrt{G_t + \epsilon}} g_t$

3. AdaData,

   $\Delta \theta_t = - \frac{\sqrt{\Delta X^2_{t-1} + \epsilon}}{\sqrt{G_t + \epsilon}}g_t$

   $\Delta X^2_{t-1} = \beta_1 \Delta X_{t-2}^2 + (1-\beta_1)\Delta \theta_{t-1} * \Delta \theta_{t-1}$

修正 $g_t$

1. 动量法    动量 = 质量 *  速度

   梯度更新延续上一次更新的趋势

   $\Delta \theta_t = \rho \Delta \theta_{t-1} - \alpha g_t$ $\rho$：质量 $\Delta \theta$ ：路程，时间默认为1，所以这就是速度

2. Adam（Adaptive Momentum） 两者都修改
   - $\alpha$：RMSprop
   - $g_t$：动量法
   - $\Delta \theta_t = - \frac{\alpha}{\sqrt{\hat{G_t + \epsilon}}} \hat{M_t}$

---







卷积层为什么需要激活函数? 激活函数实现了非线性化

- Sigmoid
- tanh
- relu



池化, 为什么要做pooling?

- 降低维数, 减少计算量
- 避免数据的偏移和抖动, 稳定数据(max , ave)

加了池化后, 为什么对反向传播没有影响?

池化的时候值没有改变, 是个常数


卷积运算填充的原理: 补0

padding作用: 控制输入输出的尺寸



mini-batch原理



normalization 原理  作用?

- 层归一化: 每一层

- 批量归一化: 针对每一个参数的每一个维度
- 参数归一化: 
- 通道归一化



避免过拟合: early stop
什么时候停止? 验证集的误差和测试集的误差

训练集 & 验证集(一般交叉验证) & 测试集 



循环神经网络拥有记忆功能: 前面历史信息向下传导

==计算公式==



输出层:softmax 输出层, 输入层, 循环层 实例..  进行计算和反向传导

RNN学习长期依赖困难? 信息保存过多, 长程依赖, 信息爆炸




激活函数的保活性. 



无监督学习



激励



![*](file:///C:/Users/DREAMB~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif) **卷积运算填充的原理**

![*](file:///C:/Users/DREAMB~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif) **分析一个现象，网络做出来之后不好，分析一下优化方法**

![*](file:///C:/Users/DREAMB~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif) **哪个东西或者哪个东西的原理/优缺点？**



---

单选 2' * 10
多选 2' * 10
判断 2' * 5
计算题 15' * 2
证明 10'
论述(简答) 5' * 2 + 10' (不需要长篇大论)